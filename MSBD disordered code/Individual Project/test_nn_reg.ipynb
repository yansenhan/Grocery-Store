{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "# We have imported all dependencied\n",
    "df = pd.read_csv('data.csv') # read data set using pandas\n",
    "print(df.info()) # Overview of dataset\n",
    "df = df.drop(['Date'],axis=1) # Drop Date feature\n",
    "df = df.dropna(inplace=False)  # Remove all nan entries.\n",
    "\n",
    "df = df.drop(['Adj Close','Volume'],axis=1) # Drop Adj close and volume feature\n",
    "df_train = df[:1059]    # 60% training data and 40% testing data\n",
    "df_test = df[1059:]\n",
    "scaler = MinMaxScaler() # For normalizing dataset\n",
    "\n",
    "# We want to predict Close value of stock \n",
    "X_train = scaler.fit_transform(df_train.drop(['Close'],axis=1).as_matrix())\n",
    "y_train = scaler.fit_transform(df_train['Close'].as_matrix())\n",
    "\n",
    "# y is output and x is features.\n",
    "X_test = scaler.fit_transform(df_test.drop(['Close'],axis=1).as_matrix())\n",
    "y_test = scaler.fit_transform(df_test['Close'].as_matrix())\n",
    "\n",
    "def denormalize(df,norm_data):\n",
    "    df = df['Close'].values.reshape(-1,1)\n",
    "    norm_data = norm_data.reshape(-1,1)\n",
    "    scl = MinMaxScaler()\n",
    "    a = scl.fit_transform(df)\n",
    "    new = scl.inverse_transform(norm_data)\n",
    "\"\"\"\n",
    "Above written function for denormalizatio of data after normalizing\n",
    "this function will give original scale of values.\n",
    "In normalization we step down the value of data in dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_model(X_data,input_dim):\n",
    "    W_1 = tf.Variable(tf.random_uniform([input_dim,10]))\n",
    "    b_1 = tf.Variable(tf.zeros([10]))\n",
    "    layer_1 = tf.add(tf.matmul(X_data,W_1), b_1)\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    # layer 1 multiplying and adding bias then activation function\n",
    "    W_2 = tf.Variable(tf.random_uniform([10,10]))\n",
    "    b_2 = tf.Variable(tf.zeros([10]))\n",
    "    layer_2 = tf.add(tf.matmul(layer_1,W_2), b_2)\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # layer 2 multiplying and adding bias then activation function\n",
    "    W_O = tf.Variable(tf.random_uniform([10,1]))\n",
    "    b_O = tf.Variable(tf.zeros([1]))\n",
    "    output = tf.add(tf.matmul(layer_2,W_O), b_O)\n",
    "    \n",
    "    # O/p layer multiplying and adding bias then activation function\n",
    "    # notice output layer has one node only since performing #regression\n",
    "    return output\n",
    "\n",
    "\"\"\"\n",
    "neural_net_model is function applying 2 hidden layer feed forward neural net.\n",
    "Weights and biases are abberviated as W_1,W_2 and b_1, b_2 \n",
    "These are variables with will be updated during training.\n",
    "\"\"\"\n",
    "xs = tf.placeholder(\"float\")\n",
    "ys = tf.placeholder(\"float\")\n",
    "output = neural_net_model(xs,3)\n",
    "cost = tf.reduce_mean(tf.square(output-ys))\n",
    "# our mean squared error cost function\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
    "# Gradinent Descent optimiztion just discussed above for updating weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initiate session and initialize all vaiables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    #saver.restore(sess,'yahoo_dataset.ckpt')\n",
    "    \n",
    "    for i in range(100):\n",
    "        for j in range(X_train.shape[0]):\n",
    "            sess.run([cost,train],feed_dict={xs:X_train[j,:].reshape(1,3), ys:y_train[j]})\n",
    "            # Run cost and train with each sample\n",
    "            \n",
    "        c_t.append(sess.run(cost, feed_dict={xs:X_train,ys:y_train}))\n",
    "        c_test.append(sess.run(cost, feed_dict={xs:X_test,ys:y_test}))\n",
    "        print('Epoch :',i,'Cost :',c_t[i])\n",
    "        \n",
    "    pred = sess.run(output, feed_dict={xs:X_test})\n",
    "    # predict output of test data after training\n",
    "    \n",
    "    print('Cost :',sess.run(cost, feed_dict={xs:X_test,ys:y_test}))\n",
    "    y_test = denormalize(df_test,y_test)\n",
    "    pred = denormalize(df_test,pred)\n",
    "    #Denormalize data\n",
    "    \n",
    "    plt.plot(range(y_test.shape[0]),y_test,label=\"Original Data\")\n",
    "    plt.plot(range(y_test.shape[0]),pred,label=\"Predicted Data\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('Stock Value')\n",
    "    plt.xlabel('Days')\n",
    "    plt.title('Stock Market Nifty')\n",
    "    plt.show()\n",
    "    \n",
    "    if input('Save model ? [Y/N]') == 'Y':\n",
    "        saver.save(sess,'yahoo_dataset.ckpt')\n",
    "        print('Model Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
